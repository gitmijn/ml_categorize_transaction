{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7511f8-7b9d-4c0f-959f-dcf4b18ac5a6",
   "metadata": {},
   "source": [
    "# Process\n",
    "1. Install llama_cpp\n",
    "2. Install llama model from Hugging Face\n",
    "3. Test model with CLI\n",
    "4. Test model from Jupyter\n",
    "  a. Import model\n",
    "  b. Add model path\n",
    "5. Import data\n",
    "6. Clean data\n",
    "7. Test LLM with fake transactions\n",
    "8. Test LLL with sample of transactions\n",
    "9. Run with full list of transaction names\n",
    "10. Relabel categories back to the data frame for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e410c2-cf86-403b-ba32-feca38d85e10",
   "metadata": {},
   "source": [
    "## Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9719afe-14cd-44ee-a411-7a862cfee1fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "8c3a472f-d5b2-44c6-9e95-ba0c43a7d3c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[457], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mixtral-8x7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(tokens, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Source/jupyter/path/to/venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Source/jupyter/path/to/venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "model.to(\"cuda\")\n",
    " \n",
    "generated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n",
    "\n",
    "# decode with mistral tokenizer\n",
    "result = tokenizer.decode(generated_ids[0].tolist())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f6ff5-4d82-4167-8127-26b1c14a6590",
   "metadata": {},
   "source": [
    "## Test connection to model by generating text\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "fc8ee910-5114-40d1-9107-dfecb9be356b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2162.49 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3236.25 ms /    22 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " independent and mysterious, yet soft and affectionate\n",
      "Eyes like lanterns in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a poem about cats.\"\n",
    "output = llm(prompt)\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc843b0-bdd1-446e-8a6a-dfbab48d73ae",
   "metadata": {},
   "source": [
    "## Read the transactions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "11cbab4e-ffd3-42e3-8edf-dbecc4216c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the credittransactions_2023_2024.csv file \n",
    "import pandas as pd\n",
    "df_cc = pd.read_csv(\"data/rawdata/expenses_2024/2024-12-14_credit_card_transaction.csv\")\n",
    "df_bc = pd.read_csv(\"data/rawdata/expenses_2024/2024-12-14_bank_card_transaction.csv\")\n",
    "df_bc['Name'] = df_bc['Name'].str.capitalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "d50648d6-8988-4813-be4f-d97e28774edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_string = ', '.join(df_cc[\"Category\"].unique().tolist()) # Convert to a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d3ff9f-1e2c-4123-9bd9-54dacf135ae8",
   "metadata": {},
   "source": [
    "## Function to categorize transactions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "9d852249-13dc-4348-8361-227b6e96bf0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique transactions in the Name / Description column\n",
    "unique_transactions = df_bc[\"Name\"].unique()\n",
    "len(unique_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "2b5f6bd3-d05a-4903-a44f-ea9d330e4774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def hop(start, end, step):\n",
    "    \"\"\"\n",
    "    Generator that yields tuples representing start and end indices for each chunk.\n",
    "    \"\"\"\n",
    "    for i in range(start, end, step):\n",
    "        yield i, min(i + step, end)\n",
    "\n",
    "def categorize_transactions(transaction_names, categories_string, llm):\n",
    "    \"\"\"Categorizes transactions with robust error handling and improved prompt.\"\"\"\n",
    "\n",
    "    system_prompt = \"You are a financial assistant. You classify expenses and income.\\n\"\n",
    "    prompt = (\n",
    "        system_prompt +\n",
    "        f\"Categories:\\n{categories_string}\\n\\n\"\n",
    "        f\"Transactions:\\n{transaction_names}\\n\\n\"\n",
    "        \"Categorize EACH transaction. Return ONLY a valid JSON list of objects, where each object has a 'Transaction' and 'Category' key. If a category can't be determined, use 'Other'. Ensure the JSON is correctly formatted. Example:\\n\"\n",
    "        '[{\"Transaction\": \"Netflix\", \"Category\": \"Entertainment\"}, {\"Transaction\": \"Unusual Transaction\", \"Category\": \"Other\"}]\\n'\n",
    "        \"Do not include any other text or explanations outside of the JSON. If you cannot produce valid JSON, return an empty JSON list:\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = llm(prompt)\n",
    "\n",
    "        if isinstance(response, dict) and 'choices' in response and response['choices']:\n",
    "            response_text = response['choices'][0]['text']\n",
    "        elif isinstance(response, str):\n",
    "            response_text = response\n",
    "        else:\n",
    "            print(f\"Unexpected LLM response format: {type(response), response}\")\n",
    "            return None\n",
    "\n",
    "        response_text = response_text.strip()\n",
    "\n",
    "        # Extract the first valid JSON-like substring\n",
    "        match = re.search(r\"\\[.*\\]\", response_text, re.DOTALL) #Find the first thing that looks like a json list\n",
    "        if match:\n",
    "            response_text = match.group(0)\n",
    "        else:\n",
    "            print(\"No JSON-like string found in LLM response\")\n",
    "            print(f\"LLM Response: {response_text}\")\n",
    "            return pd.DataFrame() #Return an empty dataframe if nothing is found\n",
    "\n",
    "        response_text = response_text.strip()\n",
    "        response_text = re.sub(r'\\n', '', response_text)\n",
    "        response_text = re.sub(r',\\s*}', '}', response_text)\n",
    "\n",
    "        try:\n",
    "            data = json.loads(response_text)\n",
    "            if not isinstance(data, list):\n",
    "                print(\"LLM did not return a list\")\n",
    "                print(f\"LLM Response: {response_text}\")\n",
    "                return None\n",
    "            for item in data:\n",
    "                if not isinstance(item, dict) or 'Transaction' not in item or 'Category' not in item:\n",
    "                    print(\"LLM returned an invalid list format\")\n",
    "                    print(f\"LLM Response: {response_text}\")\n",
    "                    return None\n",
    "            categories_df = pd.DataFrame(data)\n",
    "            return categories_df\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Invalid JSON from LLM: {e}\")\n",
    "            print(f\"LLM Response: {response_text}\")\n",
    "            return pd.DataFrame() #Return an empty dataframe if parsing fails\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "80c9d234-cff3-476a-988c-0891268b50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_transactions_chunked(unique_transactions, categories_string, llm, chunk_size=5):\n",
    "    \"\"\"\n",
    "    Categorizes transactions in chunks using an LLM and builds a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        unique_transactions: List of unique transaction names.\n",
    "        categories_string: String listing the possible categories.\n",
    "        llm: The LLM function.\n",
    "        chunk_size: Number of transactions to process in each chunk.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with Transaction and Category columns, or None on error.\n",
    "    \"\"\"\n",
    "\n",
    "    all_categorized_transactions = []\n",
    "    for start_index, end_index in hop(0, len(unique_transactions), chunk_size):\n",
    "        transaction_chunk = unique_transactions[start_index:end_index]\n",
    "        transaction_names_string = \", \".join(transaction_chunk)\n",
    "\n",
    "        try:\n",
    "            categorized_chunk = categorize_transactions(transaction_names_string, categories_string, llm)\n",
    "            if categorized_chunk is not None:\n",
    "                all_categorized_transactions.extend(categorized_chunk.to_dict('records'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error categorizing chunk {start_index} to {end_index - 1}: {e}\")\n",
    "\n",
    "    if all_categorized_transactions:\n",
    "        return pd.DataFrame(all_categorized_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "f04fc947-1414-42e6-9a07-3dfec9737159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 39 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3026.49 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2163.64 ms /   134 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid JSON from LLM: Extra data: line 1 column 3 (char 2)\n",
      "LLM Response: []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 36 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3026.49 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   122 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2286.62 ms /   137 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid JSON from LLM: Extra data: line 1 column 4 (char 3)\n",
      "LLM Response: [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 36 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3026.49 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   117 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2179.51 ms /   132 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid JSON from LLM: Extra data: line 1 column 3 (char 2)\n",
      "LLM Response: []]}}]]]]]]]]]]]]]]]]]]]]]]]]]]\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "transactions = [\"Netflix\", \"Salary\", \"Groceries\", \"Amazon Prime\", \"Rent\", \"Coffee\", \"Gas\", \"Extra Transaction 1\", \"Extra Transaction 2\", \"Extra Transaction 3\", \"Extra Transaction 4\", \"Extra Transaction 5\", \"Extra Transaction 6\"]\n",
    "unique_transactions = list(set(transactions))\n",
    "categories_string = \"Entertainment, Income, Groceries, Subscriptions, Housing, Food, Transportation, Other\"\n",
    "\n",
    "df = categorize_transactions_chunked(unique_transactions, categories_string, llm)\n",
    "if df is not None:\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "84d7246b-d9a9-4c53-b698-4a6b33f541f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "9ca7a703-3bbc-420f-b6c2-e0600ab4b990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 18 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3155.54 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2253.22 ms /   126 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid JSON from LLM: Extra data: line 1 column 3 (char 2)\n",
      "LLM Response: []]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "transactions = [\"Netflix\"]\n",
    "unique_transactions = list(set(transactions))\n",
    "categories_string = \"Entertainment, Other\"\n",
    "df = categorize_transactions_chunked(unique_transactions, categories_string, llm)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "a17120d6-ea59-498c-9883-fb7678e11444",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Llama.__call__() got an unexpected keyword argument 'system_prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[446], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m test_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```json\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming your llm function supports system prompts:\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m test_response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[0;31mTypeError\u001b[0m: Llama.__call__() got an unexpected keyword argument 'system_prompt'"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful AI assistant that only returns JSON.\"\n",
    "test_prompt = \"```json\\n[{'test': 'value'}]\\n```\"\n",
    "# Assuming your llm function supports system prompts:\n",
    "test_response = llm(test_prompt, system_prompt=system_prompt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5ed74-b53e-4d0d-9a82-ff624f141462",
   "metadata": {},
   "source": [
    "### Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b56d5d-f3f4-487c-afd2-99daa9a60275",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=h_GTxRFYETY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a79b81-14d7-41e1-82fb-81c852738f85",
   "metadata": {},
   "source": [
    "https://huggingface.co/meta-llama/Llama-3.2-1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e21ce0-a43d-4d76-a67d-6025a31f96b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
