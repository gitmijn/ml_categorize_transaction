{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a1950b3-2a45-4b19-9777-3f6be91cff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForSequenceClassification,\n",
    "  Trainer,\n",
    "  TrainingArguments,\n",
    "  pipeline\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEndpoint  # New import\n",
    "import csv\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ceee7a78-1e55-46b2-b4b0-c66c7dae051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/rawdata/expenses_2024/24_months_of_capone_transactions.csv\"\n",
    "try:\n",
    "    df_cc = pd.read_csv(\"data/rawdata/expenses_2024/24_months_of_capone_transactions.csv\")\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d14c9e2-5d24-40eb-9078-c076c28edabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to strip out prefixes and sufixes\n",
    "def preprocess_description_strip(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"^\\d+\\s\", \"\", text)\n",
    "    text = re.sub(r\"^sq\\s?\", \"\", text)\n",
    "    text = re.sub(r\"\\d+$\", \"\", text)\n",
    "    text = re.sub(r\"(amazon|walmart).*\", r\"\\1\", text)  # Keep only \"amazon\" or \"walmart\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30603f4b-c59e-4661-b495-631716d8a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to normalize the descriptions\n",
    "\n",
    "def preprocess_description_train(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers (adjust if needed)\n",
    "    text = text.strip()  # Remove extra whitespace\n",
    "    # Add more specific grouping/truncation logic here based on your review\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83593b06-98d2-4ea6-8a6e-84f6ff54723e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label: {0: 'Airfare', 1: 'Car Rental', 2: 'Dining', 3: 'Entertainment', 4: 'Gas/Automotive', 5: 'Health Care', 6: 'Healthcare', 7: 'Insurance', 8: 'Internet', 9: 'Lodging', 10: 'Merchandise', 11: 'Other', 12: 'Other Services', 13: 'Other Travel', 14: 'Payment/Credit', 15: 'Phone/Cable', 16: 'Professional Services', 17: 'Utilities'}\n",
      "label2id: {'Airfare': 0, 'Car Rental': 1, 'Dining': 2, 'Entertainment': 3, 'Gas/Automotive': 4, 'Health Care': 5, 'Healthcare': 6, 'Insurance': 7, 'Internet': 8, 'Lodging': 9, 'Merchandise': 10, 'Other': 11, 'Other Services': 12, 'Other Travel': 13, 'Payment/Credit': 14, 'Phone/Cable': 15, 'Professional Services': 16, 'Utilities': 17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/hzpm2w797xgc9cbrbwxtssfr0000gp/T/ipykernel_11773/2235656167.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cc_unique['Description'] = df_cc_unique['Description'].apply(preprocess_description_strip)\n",
      "/var/folders/j1/hzpm2w797xgc9cbrbwxtssfr0000gp/T/ipykernel_11773/2235656167.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cc_unique['Description'] = df_cc_unique['Description'].apply(preprocess_description_train)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "df_cc = df_cc[[\"Description\", \"Category\"]].dropna()  # Select relevant columns and drop missing values\n",
    "df_cc_unique = df_cc.drop_duplicates()\n",
    "df_cc_unique['Description'] = df_cc_unique['Description'].apply(preprocess_description_strip)\n",
    "df_cc_unique['Description'] = df_cc_unique['Description'].apply(preprocess_description_train)\n",
    "df_cc_unique = df_cc_unique.rename(columns={\"Description\": \"text\", \"Category\": \"labels\"}) # Rename columns for consistency\n",
    "df_cc_unique.to_csv(\"training_categories.csv\", index=False)\n",
    "\n",
    "# Convert labels to category type\n",
    "df_cc_unique[\"labels\"] = df_cc_unique[\"labels\"].astype(\"category\")\n",
    "\n",
    "# Create the mappings\n",
    "id2label = dict(enumerate(df_cc_unique[\"labels\"].cat.categories))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Convert labels to numerical codes\n",
    "df_cc_unique[\"labels\"] = df_cc_unique[\"labels\"].cat.codes\n",
    "\n",
    "# Define the labels variable\n",
    "labels = df_cc_unique[\"labels\"].unique()\n",
    "\n",
    "# Display the mappings and the processed DataFrame\n",
    "print(\"id2label:\", id2label)\n",
    "print(\"label2id:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e0269b7-53ed-4e2b-9a86-cddbe614354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_df, eval_df = train_test_split(df_cc_unique, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert Pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab60bb-d709-4ccc-b1d9-1b08f84c3dc2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82e6090a-605e-4f90-a7a8-eaab6013e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db051746-b16b-4904-912e-eb4869a5e862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:00<00:00, 21346.01 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:00<00:00, 28268.13 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/Holden/Source/jupyter/.venv/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 02:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.624931</td>\n",
       "      <td>0.554913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.334246</td>\n",
       "      <td>0.624277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.276715</td>\n",
       "      <td>0.635838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.210516</td>\n",
       "      <td>0.676301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.212929</td>\n",
       "      <td>0.693642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Load model with label mappings\n",
    "num_labels = len(labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "# 3. Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "\n",
    "# 4. Tokenize training and eval datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 5. Load model with label mappings\n",
    "num_labels = len(labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# 6. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=25,  # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=25,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# 7. Define metrics\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 8. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 9. Train model\n",
    "trainer.train()\n",
    "\n",
    "# 10. Evaluate model\n",
    "trainer.evaluate()\n",
    "\n",
    "# 11. Save model\n",
    "trainer.save_model(\"fine_tuned_transaction_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd362c40-1712-4b83-baee-52c7f6ee3228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to ID mapping: {'Airfare': 0, 'Car Rental': 1, 'Dining': 2, 'Entertainment': 3, 'Gas/Automotive': 4, 'Health Care': 5, 'Healthcare': 6, 'Insurance': 7, 'Internet': 8, 'Lodging': 9, 'Merchandise': 10, 'Other': 11, 'Other Services': 12, 'Other Travel': 13, 'Payment/Credit': 14, 'Phone/Cable': 15, 'Professional Services': 16, 'Utilities': 17}\n",
      "ID to Label mapping: {0: 'Airfare', 1: 'Car Rental', 2: 'Dining', 3: 'Entertainment', 4: 'Gas/Automotive', 5: 'Health Care', 6: 'Healthcare', 7: 'Insurance', 8: 'Internet', 9: 'Lodging', 10: 'Merchandise', 11: 'Other', 12: 'Other Services', 13: 'Other Travel', 14: 'Payment/Credit', 15: 'Phone/Cable', 16: 'Professional Services', 17: 'Utilities'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Label to ID mapping:\", label2id)\n",
    "print(\"ID to Label mapping:\", id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7603f90-cfe4-4d5f-8150-56d89252dcc6",
   "metadata": {},
   "source": [
    "### Predict Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ba01385-51a2-41b7-b1f9-752641759ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load new data (assuming it's in a CSV called 'new_transactions.csv')\n",
    "new_data = pd.read_csv(\"data/rawdata/expenses_2024/2024-12-14_bank_card_transaction.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "390d4eac-4d1c-4116-b27f-8d70e1d54be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Holden/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Cleaned_Name Predicted_Category\n",
      "0             py coal yard cafe ithaca ny             Dining\n",
      "1                     rens mart ithaca ny        Merchandise\n",
      "2                       usps po ithaca ny     Other Services\n",
      "3                   fedex offic ithaca ny     Other Services\n",
      "4                        target ithaca ny     Other Services\n",
      "5                             etsy inc ny        Merchandise\n",
      "6                google one ai premium ca        Phone/Cable\n",
      "7               cayuga medical associa ny        Health Care\n",
      "8                                  amazon        Merchandise\n",
      "9   carnegie mellon univer httpswwwcmu pa      Entertainment\n",
      "10                                 amazon        Merchandise\n",
      "11                                 amazon        Merchandise\n",
      "12                          payment thank        Merchandise\n",
      "13            courseraorg httpswwwcour ca      Entertainment\n",
      "14                                 amazon        Merchandise\n",
      "15           collegetown bagels ithaca ny     Other Services\n",
      "16                                 amazon        Merchandise\n",
      "17                                 amazon        Merchandise\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Define the text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove \"TST*\" prefix\n",
    "    text = re.sub(r'^TST\\*', '', text)\n",
    "    \n",
    "    # Remove numeric codes\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove unwanted characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers (adjust if needed)\n",
    "    text = re.sub(r\"^\\d+\\s\", \"\", text)\n",
    "    text = re.sub(r\"^sq\\s?\", \"\", text)\n",
    "    text = re.sub(r\"\\d+$\", \"\", text)\n",
    "    text = re.sub(r\"(amazon|walmart).*\", r\"\\1\", text)  # Keep only \"amazon\" or \"walmart\"\n",
    "    \n",
    "    # Remove double quotes\n",
    "    text = text.replace('\"', '')\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove stop words (optional)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Load the new data\n",
    "new_data = pd.read_csv(\"data/rawdata/expenses_2024/2024-12-14_bank_card_transaction.csv\")\n",
    "\n",
    "# Apply the cleaning function to the 'Name' column\n",
    "new_data[\"Cleaned_Name\"] = new_data[\"Name\"].apply(clean_text)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"fine_tuned_transaction_classifier\")\n",
    "\n",
    "# Define the id2label dictionary based on your training data\n",
    "# Ensure this dictionary matches the one used during training\n",
    "\n",
    "\n",
    "# Create a pipeline for prediction (set function_to_apply to 'softmax')\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, function_to_apply='softmax')\n",
    "\n",
    "# Make predictions using the cleaned text data\n",
    "predictions = classifier(new_data[\"Cleaned_Name\"].tolist())\n",
    "\n",
    "# Extract the predicted labels (numerical codes)\n",
    "#predicted_codes = [int(prediction['label'].replace('LABEL_', '')) for prediction in predictions]\n",
    "\n",
    "# Extract labels from the predictions\n",
    "labels = [prediction['label'] for prediction in predictions]\n",
    "\n",
    "# Add the labels as a new column in the DataFrame\n",
    "new_data['Predicted_Category'] = labels\n",
    "\n",
    "# Print or save the updated DataFrame\n",
    "print(new_data[['Cleaned_Name', 'Predicted_Category']])\n",
    "# new_data.to_csv(\"categorized_transactions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6df27c-6619-4c92-b08e-b7e478fb4934",
   "metadata": {},
   "source": [
    "### Predict Full 2024 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4aa6e144-d060-45d9-a4dd-06ae2ddf77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load 2024 Data\n",
    "full_data = pd.read_csv(\"data/rawdata/expenses_2024/2024-12-17_credit_card_12_31_2023_2024.csv\")\n",
    "# 2. Apply the cleaning function to the 'Name' column\n",
    "full_data[\"Cleaned_Name\"] = full_data[\"Name\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7adc289-afbd-409b-82b6-55545dfec7ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# 3. Load the tokenizer and model same as before\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"fine_tuned_transaction_classifier\")\n",
    "\n",
    "# 4. Create a new pipeline for prediction (set function_to_apply to 'softmax')\n",
    "classifier_full = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, function_to_apply='softmax')\n",
    "\n",
    "# Make predictions using the cleaned text data\n",
    "predictions_full = classifier_full(full_data[\"Cleaned_Name\"].tolist())\n",
    "\n",
    "# Extract the predicted labels (numerical codes)\n",
    "#predicted_codes = [int(predictions_full['label'].replace('LABEL_', '')) for prediction in predictions]\n",
    "\n",
    "# Extract labels from the predictions_full\n",
    "labels_full = [prediction['label'] for prediction in predictions_full]\n",
    "\n",
    "# Add the labels as a new column in the DataFrame\n",
    "full_data['Predicted_Category'] = labels_full\n",
    "\n",
    "# (Optional) Print or save the updated DataFrame\n",
    "#full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4b97c6e5-45ee-46b8-8a76-3153f643c4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned_Name</th>\n",
       "      <th>Predicted_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>adams corners caf ithaca ny</td>\n",
       "      <td>Dining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>amazon</td>\n",
       "      <td>Merchandise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>amzn mktp usbsku amzncombill wa</td>\n",
       "      <td>Merchandise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>amzn mktp usddota amzncombill wa</td>\n",
       "      <td>Merchandise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>amzn mktp usrae amzncombill wa</td>\n",
       "      <td>Merchandise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Cleaned_Name Predicted_Category\n",
       "43       adams corners caf ithaca ny             Dining\n",
       "41                            amazon        Merchandise\n",
       "25   amzn mktp usbsku amzncombill wa        Merchandise\n",
       "16  amzn mktp usddota amzncombill wa        Merchandise\n",
       "12    amzn mktp usrae amzncombill wa        Merchandise"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data[['Cleaned_Name', 'Predicted_Category']].drop_duplicates().sort_values(by='Cleaned_Name').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd3b0f-c2a2-4a4c-a7ee-ae8af6295a62",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d56efa-955b-4aa5-9954-0056f70f0048",
   "metadata": {},
   "source": [
    "This worked pretty well overall. Having a larger training dataset would have been ideal but that would mean that I would have to spend a lot more money :D or get a third party dataset. Overall, I like this fine tuning approach to local. \n",
    "\n",
    "#### Next steps\n",
    "- On this would be to see if an LLM can do better\n",
    "- An evaluation of the category\n",
    "- Compile all the data and see what the difference is in spending per category\n",
    "\n",
    "#### Notable issues\n",
    "- Some notable exceptions are Apple.com was categrized as dining.\n",
    "- A lot of things get categorized into Amazon Merchandise\n",
    "- I should be scrubbing out payments and credits\n",
    "\n",
    "I tried increasing the number of epochs but that did not seem to make an observable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89046339-cda6-4acf-823b-c88357ae244b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
